"""
Google APIé›†æˆæ¨¡å—
æä¾›Googleæœç´¢ã€Gmailã€Google Mapsç­‰APIåŠŸèƒ½
"""

from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, EmailStr
import httpx
import os
import logging
import json
from typing import Optional, Dict, List, Any
from urllib.parse import quote
import re
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter(prefix="/api/google", tags=["Google API"])

class GoogleSearchRequest(BaseModel):
    query: str
    num_results: Optional[int] = 10
    language: Optional[str] = "zh-CN"

class EmailAnalysisRequest(BaseModel):
    email: EmailStr
    include_social: Optional[bool] = True
    include_maps: Optional[bool] = True

class GoogleAnalysisResponse(BaseModel):
    email: str
    google_account_exists: bool
    profile_info: Dict[str, Any]
    maps_data: Dict[str, Any]
    social_profiles: List[Dict[str, Any]]
    privacy_score: str
    risk_assessment: str
    analysis_timestamp: str

# Googleæœç´¢ç›¸å…³é…ç½®
GOOGLE_SEARCH_ENGINES = {
    "custom_search": "https://www.googleapis.com/customsearch/v1",
    "serpapi": "https://serpapi.com/search.json",
    "duckduckgo": "https://api.duckduckgo.com/"
}

# GHunté£æ ¼çš„ç”¨æˆ·ä»£ç†
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
]

@router.post("/search")
async def google_search(request: GoogleSearchRequest):
    """
    æ‰§è¡ŒGoogleæœç´¢
    """
    try:
        logger.info(f"ğŸ” Performing Google search for: {request.query}")
        
        # ä½¿ç”¨DuckDuckGoä½œä¸ºå¤‡ç”¨æœç´¢å¼•æ“ï¼ˆé¿å…Google APIé…é¢é™åˆ¶ï¼‰
        search_results = await perform_duckduckgo_search(
            query=request.query,
            num_results=request.num_results
        )
        
        return {
            "success": True,
            "query": request.query,
            "results": search_results,
            "total_results": len(search_results),
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Google search error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Google search failed: {str(e)}")

@router.post("/analyze")
async def analyze_google_account(request: EmailAnalysisRequest):
    """
    åˆ†æGoogleè´¦æˆ·ä¿¡æ¯ - ä½¿ç”¨æ–°çš„é‚®ç®±è°ƒæŸ¥API
    """
    try:
        logger.info(f"ğŸ” [Google API] Starting investigation for: {request.email}")
        
        # å¤–éƒ¨é‚®ç®±è°ƒæŸ¥APIé…ç½®
        INVESTIGATE_API_URL = os.getenv("GOOGLE_EMAIL_API_URL", "http://47.253.47.192:8002/api/email")
        REQUEST_TIMEOUT = 60
        
        # å‡†å¤‡è¯·æ±‚æ•°æ®
        payload = {
            "email": str(request.email)
        }
        
        logger.info(f"ğŸŒ [Google API] Target URL: {INVESTIGATE_API_URL}")
        logger.info(f"ğŸ“§ [Google API] Email: {request.email}")
        
        # è°ƒç”¨å¤–éƒ¨è°ƒæŸ¥API
        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT) as client:
            try:
                headers = {
                    "Content-Type": "application/json",
                    "Accept": "application/json"
                }
                
                # å…¼å®¹ä¸¤ç§å¤–éƒ¨æ¥å£ï¼š8002/api/email ä¸ 8000/api/v1/email/investigate
                if "/api/email" in INVESTIGATE_API_URL:
                    response = await client.post(
                        INVESTIGATE_API_URL,
                        json=payload,
                        headers=headers
                    )
                else:
                    response = await client.post(
                        INVESTIGATE_API_URL,
                        json=payload,
                        headers=headers
                    )
                
                logger.info(f"ğŸ“¡ [Google API] API response status: {response.status_code}")
                
                if response.status_code != 200:
                    error_text = response.text
                    logger.error(f"âŒ [Google API] API error {response.status_code}: {error_text}")
                    
                    # å¦‚æœå¤–éƒ¨APIå¤±è´¥ï¼Œè¿”å›å‹å¥½çš„å…¬å¼€æ•°æ®æœç´¢ç»“æœ
                    fallback_data = await fallback_email_investigation(request.email)
                    
                    # è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼ï¼Œæ ‡è®°ä¸ºå…¬å¼€æ•°æ®æœç´¢æ¨¡å¼
                    return {
                        "email": str(request.email),
                        "step1_registration": {
                            "account_exists": True,  # æ ‡è®°ä¸ºå­˜åœ¨ä»¥æ˜¾ç¤ºå¡ç‰‡
                            "gaia_id": None,
                            "status": "public_data_only",  # ç‰¹æ®ŠçŠ¶æ€
                            "note": "å¤–éƒ¨APIä¸å¯ç”¨ï¼Œä½¿ç”¨å…¬å¼€æ•°æ®æœç´¢"
                        },
                        "step2_people_info": {
                            "name": fallback_data.get("profile_data", {}).get("name"),
                            "person_id": None,
                            "source_ids": []
                        },
                        "step3_additional_data": {
                            "social_accounts": fallback_data.get("social_accounts", []),
                            "digital_footprint": fallback_data.get("digital_footprint", {}),
                            "profile_data": fallback_data.get("profile_data", {}),
                            "raw_response": fallback_data
                        },
                        "step4_summary": {
                            "total_data_sources": len(fallback_data.get("social_accounts", [])),
                            "privacy_exposure": f"å…¬å¼€æ•°æ®æœç´¢: æ‰¾åˆ°{len(fallback_data.get('social_accounts', []))}ä¸ªç›¸å…³ç»“æœ"
                        },
                        "step5_location_analysis": {
                            "location_data": {},
                            "maps_data": {}
                        },
                        "step6_reverse_image": {
                            "avatar_analysis": "ä½¿ç”¨å…¬å¼€æ•°æ®æœç´¢",
                            "reverse_search_results": []
                        },
                        "avatar_url": fallback_data.get("profile_data", {}).get("avatar_url", ""),
                        "analysis_timestamp": datetime.utcnow().isoformat(),
                        "privacy_score": "UNKNOWN",
                        "total_data_points": len(fallback_data.get("social_accounts", [])),
                        "overall_risk_level": "UNKNOWN",
                        "api_version": "fallback_v1",
                        "mode": "public_data_search",
                        "external_api_status": "unavailable",
                        "raw_api_response": fallback_data
                    }
                
                # å°è¯•è§£æå“åº”
                try:
                    api_data = response.json()
                    logger.info(f"âœ… [Google API] Successfully parsed JSON response")
                    logger.info(f"ğŸ“Š [Google API] Response keys: {list(api_data.keys()) if isinstance(api_data, dict) else 'Not a dict'}")
                except json.JSONDecodeError as json_err:
                    logger.error(f"âŒ [Google API] JSON decode error: {str(json_err)}")
                    logger.error(f"ğŸ“‹ [Google API] Raw response: {response.text[:500]}...")
                    
                    # JSONè§£æå¤±è´¥ï¼Œè¿”å›å‹å¥½çš„å…¬å¼€æ•°æ®æœç´¢ç»“æœ
                    fallback_data = await fallback_email_investigation(request.email)
                    
                    return {
                        "email": str(request.email),
                        "step1_registration": {
                            "account_exists": True,
                            "gaia_id": None,
                            "status": "public_data_only",
                            "note": "å¤–éƒ¨APIå“åº”æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨å…¬å¼€æ•°æ®æœç´¢"
                        },
                        "step2_people_info": {
                            "name": fallback_data.get("profile_data", {}).get("name"),
                            "person_id": None,
                            "source_ids": []
                        },
                        "step3_additional_data": {
                            "social_accounts": fallback_data.get("social_accounts", []),
                            "digital_footprint": fallback_data.get("digital_footprint", {}),
                            "profile_data": fallback_data.get("profile_data", {}),
                            "raw_response": fallback_data
                        },
                        "step4_summary": {
                            "total_data_sources": len(fallback_data.get("social_accounts", [])),
                            "privacy_exposure": f"å…¬å¼€æ•°æ®æœç´¢: æ‰¾åˆ°{len(fallback_data.get('social_accounts', []))}ä¸ªç›¸å…³ç»“æœ"
                        },
                        "step5_location_analysis": {
                            "location_data": {},
                            "maps_data": {}
                        },
                        "step6_reverse_image": {
                            "avatar_analysis": "ä½¿ç”¨å…¬å¼€æ•°æ®æœç´¢",
                            "reverse_search_results": []
                        },
                        "avatar_url": fallback_data.get("profile_data", {}).get("avatar_url", ""),
                        "analysis_timestamp": datetime.utcnow().isoformat(),
                        "privacy_score": "UNKNOWN",
                        "total_data_points": len(fallback_data.get("social_accounts", [])),
                        "overall_risk_level": "UNKNOWN",
                        "api_version": "fallback_v1",
                        "mode": "public_data_search",
                        "external_api_status": "json_error",
                        "raw_api_response": fallback_data
                    }
                
            except httpx.HTTPStatusError as http_err:
                logger.error(f"âŒ [Google API] HTTP error: {str(http_err)}")
                fallback_data = await fallback_email_investigation(request.email)
                return {
                    "email": str(request.email),
                    "step1_registration": {
                        "account_exists": True,
                        "gaia_id": None,
                        "status": "public_data_only",
                        "note": "HTTPé”™è¯¯ï¼Œä½¿ç”¨å…¬å¼€æ•°æ®æœç´¢"
                    },
                    "step2_people_info": {"name": fallback_data.get("profile_data", {}).get("name"), "person_id": None, "source_ids": []},
                    "step3_additional_data": {"social_accounts": fallback_data.get("social_accounts", []), "digital_footprint": fallback_data.get("digital_footprint", {}), "profile_data": fallback_data.get("profile_data", {}), "raw_response": fallback_data},
                    "step4_summary": {"total_data_sources": len(fallback_data.get("social_accounts", [])), "privacy_exposure": f"å…¬å¼€æ•°æ®æœç´¢: æ‰¾åˆ°{len(fallback_data.get('social_accounts', []))}ä¸ªç›¸å…³ç»“æœ"},
                    "step5_location_analysis": {"location_data": {}, "maps_data": {}},
                    "step6_reverse_image": {"avatar_analysis": "ä½¿ç”¨å…¬å¼€æ•°æ®æœç´¢", "reverse_search_results": []},
                    "avatar_url": fallback_data.get("profile_data", {}).get("avatar_url", ""),
                    "analysis_timestamp": datetime.utcnow().isoformat(),
                    "privacy_score": "UNKNOWN",
                    "total_data_points": len(fallback_data.get("social_accounts", [])),
                    "overall_risk_level": "UNKNOWN",
                    "api_version": "fallback_v1",
                    "mode": "public_data_search",
                    "external_api_status": "http_error",
                    "raw_api_response": fallback_data
                }
            except Exception as req_err:
                logger.error(f"âŒ [Google API] Request error: {str(req_err)}")
                fallback_data = await fallback_email_investigation(request.email)
                return {
                    "email": str(request.email),
                    "step1_registration": {
                        "account_exists": True,
                        "gaia_id": None,
                        "status": "public_data_only",
                        "note": "è¯·æ±‚é”™è¯¯ï¼Œä½¿ç”¨å…¬å¼€æ•°æ®æœç´¢"
                    },
                    "step2_people_info": {"name": fallback_data.get("profile_data", {}).get("name"), "person_id": None, "source_ids": []},
                    "step3_additional_data": {"social_accounts": fallback_data.get("social_accounts", []), "digital_footprint": fallback_data.get("digital_footprint", {}), "profile_data": fallback_data.get("profile_data", {}), "raw_response": fallback_data},
                    "step4_summary": {"total_data_sources": len(fallback_data.get("social_accounts", [])), "privacy_exposure": f"å…¬å¼€æ•°æ®æœç´¢: æ‰¾åˆ°{len(fallback_data.get('social_accounts', []))}ä¸ªç›¸å…³ç»“æœ"},
                    "step5_location_analysis": {"location_data": {}, "maps_data": {}},
                    "step6_reverse_image": {"avatar_analysis": "ä½¿ç”¨å…¬å¼€æ•°æ®æœç´¢", "reverse_search_results": []},
                    "avatar_url": fallback_data.get("profile_data", {}).get("avatar_url", ""),
                    "analysis_timestamp": datetime.utcnow().isoformat(),
                    "privacy_score": "UNKNOWN",
                    "total_data_points": len(fallback_data.get("social_accounts", [])),
                    "overall_risk_level": "UNKNOWN",
                    "api_version": "fallback_v1",
                    "mode": "public_data_search",
                    "external_api_status": "request_error",
                    "raw_api_response": fallback_data
                }
            
            # å¤„ç†è°ƒæŸ¥APIçš„å“åº”æ•°æ®ï¼Œè½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
            # å¤–éƒ¨APIè¿”å›æ ¼å¼: {status, message, data: {person_id, email, profile_photo, etc}, maps_stats: {reviews, photos, etc}}
            # ç»Ÿä¸€è§£æä¸åŒAPIç»“æ„
            user_data = api_data.get("data", {}) if isinstance(api_data, dict) else {}
            maps_stats = api_data.get("maps_stats", {}) if isinstance(api_data, dict) else {}
            status = api_data.get("status") if isinstance(api_data, dict) else None

            # å…¼å®¹ 8002/api/email è¿”å›ç»“æ„ï¼šé¡¶å±‚ PROFILE_CONTAINER
            if isinstance(api_data, dict) and not user_data and "PROFILE_CONTAINER" in api_data:
                pc = api_data.get("PROFILE_CONTAINER", {})
                profile = pc.get("profile", {})
                profile_infos = (profile.get("profileInfos", {}) or {}).get("PROFILE", {})
                apps_info = (profile.get("inAppReachability", {}) or {}).get("PROFILE", {})
                activated_services = []
                if isinstance(profile_infos.get("userTypes"), list):
                    activated_services.extend(profile_infos.get("userTypes") or [])
                if isinstance(apps_info.get("apps"), list):
                    activated_services.extend(apps_info.get("apps") or [])
                maps = pc.get("maps", {})
                maps_stats = maps.get("stats", {})
                photo = (profile.get("profilePhotos", {}) or {}).get("PROFILE", {})
                user_data = {
                    "person_id": profile.get("personId"),
                    "name": None,
                    "profile_photo": photo.get("url"),
                    "activated_services": activated_services,
                    "location_data": {},
                }
                status = "success" if user_data.get("person_id") else status

            has_account = bool(user_data.get("person_id") or user_data.get("name") or status == "success")
            
            result = {
                "email": str(request.email),
                "step1_registration": {
                    "account_exists": has_account,
                    "gaia_id": user_data.get("person_id"),
                    "status": "active" if has_account else "not_found"
                },
                "step2_people_info": {
                    "name": user_data.get("name", ""),
                    "person_id": user_data.get("person_id"),
                    "source_ids": user_data.get("activated_services", [])
                },
                "step3_additional_data": {
                    "social_accounts": user_data.get("activated_services", []),
                    "digital_footprint": maps_stats,
                    "profile_data": user_data,
                    "raw_response": api_data
                },
                "step4_summary": {
                    "total_data_sources": len(user_data.get("activated_services", [])) + (1 if maps_stats else 0),
                    "privacy_exposure": f"Found {len(user_data.get('activated_services', []))} activated services" if has_account else "No account found"
                },
                "step5_location_analysis": {
                    "location_data": user_data.get("location_data", {}),
                    "maps_data": maps_stats
                },
                "step6_reverse_image": {
                    "avatar_analysis": "Profile photo available" if user_data.get("profile_photo") else "No profile photo found",
                    "reverse_search_results": []
                },
                "avatar_url": user_data.get("profile_photo", ""),
                "analysis_timestamp": datetime.utcnow().isoformat(),
                "privacy_score": "HIGH" if has_account and maps_stats else "MEDIUM" if has_account else "LOW",
                "total_data_points": len(user_data.get("activated_services", [])) + sum([maps_stats.get("reviews", 0), maps_stats.get("photos", 0), maps_stats.get("answers", 0)]),
                "overall_risk_level": "HIGH" if isinstance(maps_stats, dict) and (maps_stats.get("Reviews") or maps_stats.get("reviews") or 0) > 0 else "MODERATE" if has_account else "LOW",
                "api_version": "investigate_v1",
                "raw_api_response": api_data
            }
            
            logger.info(f"âœ… [Google API] Investigation completed successfully for {request.email}")
            return result
            
    except httpx.TimeoutException:
        error_msg = f"Googleé‚®ç®±è°ƒæŸ¥è¯·æ±‚è¶…æ—¶ï¼ˆè¶…è¿‡{REQUEST_TIMEOUT}ç§’ï¼‰"
        logger.error(f"â° [Google API] {error_msg}")
        raise HTTPException(status_code=408, detail=error_msg)
        
    except httpx.RequestError as e:
        error_msg = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
        logger.error(f"ğŸŒ [Google API] {error_msg}")
        raise HTTPException(status_code=503, detail=error_msg)
        
    except Exception as e:
        error_msg = f"Googleé‚®ç®±è°ƒæŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e) if str(e) else 'æœªçŸ¥é”™è¯¯'}"
        logger.error(f"ğŸ’¥ [Google API] {error_msg}")
        logger.exception("Full exception details:")
        raise HTTPException(status_code=500, detail=error_msg)

@router.post("/investigate")
async def investigate_google_email(request: EmailAnalysisRequest):
    """
    è°ƒæŸ¥Googleé‚®ç®±ä¿¡æ¯ - ä½¿ç”¨å¤–éƒ¨è°ƒæŸ¥API
    """
    try:
        logger.info(f"ğŸ” [Google Investigate] Starting investigation for: {request.email}")
        
        # å¤–éƒ¨é‚®ç®±è°ƒæŸ¥APIé…ç½®
        INVESTIGATE_API_URL = "http://47.253.47.192:8000/api/v1/email/investigate"
        REQUEST_TIMEOUT = 60
        
        # å‡†å¤‡è¯·æ±‚æ•°æ®
        payload = {
            "email": str(request.email)
        }
        
        logger.info(f"ğŸŒ [Google Investigate] Target URL: {INVESTIGATE_API_URL}")
        logger.info(f"ğŸ“§ [Google Investigate] Email: {request.email}")
        
        # è°ƒç”¨å¤–éƒ¨è°ƒæŸ¥API
        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT) as client:
            try:
                headers = {
                    "Content-Type": "application/json",
                    "Accept": "application/json"
                }
                
                response = await client.post(
                    INVESTIGATE_API_URL,
                    json=payload,
                    headers=headers
                )
                
                logger.info(f"ğŸ“¡ [Google Investigate] API response status: {response.status_code}")
                
                if response.status_code != 200:
                    error_text = response.text
                    logger.error(f"âŒ [Google Investigate] API error {response.status_code}: {error_text}")
                    
                    # å¦‚æœå¤–éƒ¨APIå¤±è´¥ï¼Œè¿”å›åŸºæœ¬çš„é‚®ç®±æ£€æŸ¥ç»“æœ
                    return await fallback_email_investigation(request.email)
                
                # å°è¯•è§£æå“åº”
                try:
                    api_data = response.json()
                    logger.info(f"âœ… [Google Investigate] Successfully parsed JSON response")
                    logger.info(f"ğŸ“Š [Google Investigate] Response keys: {list(api_data.keys()) if isinstance(api_data, dict) else 'Not a dict'}")
                except json.JSONDecodeError as json_err:
                    logger.error(f"âŒ [Google Investigate] JSON decode error: {str(json_err)}")
                    logger.error(f"ğŸ“‹ [Google Investigate] Raw response: {response.text[:500]}...")
                    
                    # JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                    return await fallback_email_investigation(request.email)
                
            except httpx.HTTPStatusError as http_err:
                logger.error(f"âŒ [Google Investigate] HTTP error: {str(http_err)}")
                return await fallback_email_investigation(request.email)
            except Exception as req_err:
                logger.error(f"âŒ [Google Investigate] Request error: {str(req_err)}")
                return await fallback_email_investigation(request.email)
            
            # å¤„ç†è°ƒæŸ¥APIçš„å“åº”æ•°æ®
            result = {
                "email": str(request.email),
                "investigation_status": "completed",
                "data_sources": api_data.get("sources", []),
                "profile_data": api_data.get("profile", {}),
                "social_accounts": api_data.get("social_accounts", []),
                "digital_footprint": api_data.get("digital_footprint", {}),
                "privacy_analysis": {
                    "exposure_level": api_data.get("privacy", {}).get("exposure_level", "unknown"),
                    "risk_score": api_data.get("privacy", {}).get("risk_score", 0),
                    "recommendations": api_data.get("privacy", {}).get("recommendations", [])
                },
                "metadata": {
                    "investigation_timestamp": datetime.utcnow().isoformat(),
                    "api_version": "v1",
                    "processing_time": api_data.get("processing_time", "unknown"),
                    "data_quality": api_data.get("data_quality", "unknown")
                },
                "raw_api_response": api_data
            }
            
            logger.info(f"âœ… [Google Investigate] Investigation completed successfully for {request.email}")
            return result
            
    except httpx.TimeoutException:
        error_msg = f"Googleé‚®ç®±è°ƒæŸ¥è¯·æ±‚è¶…æ—¶ï¼ˆè¶…è¿‡{REQUEST_TIMEOUT}ç§’ï¼‰"
        logger.error(f"â° [Google Investigate] {error_msg}")
        raise HTTPException(status_code=408, detail=error_msg)
        
    except httpx.RequestError as e:
        error_msg = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
        logger.error(f"ğŸŒ [Google Investigate] {error_msg}")
        raise HTTPException(status_code=503, detail=error_msg)
        
    except Exception as e:
        error_msg = f"Googleé‚®ç®±è°ƒæŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e) if str(e) else 'æœªçŸ¥é”™è¯¯'}"
        logger.error(f"ğŸ’¥ [Google Investigate] {error_msg}")
        logger.exception("Full exception details:")
        raise HTTPException(status_code=500, detail=error_msg)

@router.get("/maps/reviews")
async def get_maps_reviews(
    gaia_id: str = Query(..., description="Google Gaia ID"),
    max_reviews: int = Query(10, description="Maximum number of reviews to fetch")
):
    """
    è·å–ç”¨æˆ·çš„Google Mapsè¯„è®º
    """
    try:
        logger.info(f"ğŸ—ºï¸ Fetching Google Maps reviews for Gaia ID: {gaia_id}")
        
        # æ„å»ºGoogle Maps APIè¯·æ±‚
        reviews_data = await fetch_google_maps_reviews(gaia_id, max_reviews)
        
        return {
            "success": True,
            "gaia_id": gaia_id,
            "reviews": reviews_data,
            "total_reviews": len(reviews_data),
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Google Maps reviews error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch Google Maps reviews: {str(e)}")

@router.get("/profile/avatar")
async def get_google_avatar(
    email: str = Query(..., description="Email address"),
    size: int = Query(200, description="Avatar size in pixels")
):
    """
    è·å–Googleè´¦æˆ·å¤´åƒ
    """
    try:
        logger.info(f"ğŸ‘¤ Fetching Google avatar for: {email}")
        
        # å°è¯•è·å–Googleå¤´åƒ
        avatar_info = await fetch_google_avatar(email, size)
        
        return {
            "success": True,
            "email": email,
            "avatar_url": avatar_info.get("url"),
            "is_default": avatar_info.get("is_default", True),
            "size": size,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Google avatar error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch Google avatar: {str(e)}")

# ==================== è¾…åŠ©å‡½æ•° ====================

async def perform_duckduckgo_search(query: str, num_results: int = 10) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨DuckDuckGoæ‰§è¡Œæœç´¢
    """
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            # DuckDuckGoå³æ—¶æœç´¢API
            url = "https://api.duckduckgo.com/"
            params = {
                "q": query,
                "format": "json",
                "no_html": "1",
                "skip_disambig": "1"
            }
            
            headers = {
                "User-Agent": USER_AGENTS[0]
            }
            
            response = await client.get(url, params=params, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            # å¤„ç†ç›¸å…³ä¸»é¢˜
            for topic in data.get("RelatedTopics", [])[:num_results]:
                if isinstance(topic, dict) and "Text" in topic:
                    results.append({
                        "title": topic.get("Text", "").split(" - ")[0] if " - " in topic.get("Text", "") else topic.get("Text", ""),
                        "description": topic.get("Text", ""),
                        "url": topic.get("FirstURL", ""),
                        "source": "DuckDuckGo"
                    })
            
            return results[:num_results]
            
    except Exception as e:
        logger.error(f"DuckDuckGo search error: {str(e)}")
        return []

async def check_google_account_existence(email: str) -> Dict[str, Any]:
    """
    æ£€æŸ¥Googleè´¦æˆ·æ˜¯å¦å­˜åœ¨
    """
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            # ä½¿ç”¨Googleè´¦æˆ·æ¢å¤é¡µé¢æ£€æŸ¥è´¦æˆ·å­˜åœ¨æ€§
            url = "https://accounts.google.com/signin/v2/lookup"
            
            data = {
                "Email": email,
                "continue": "https://accounts.google.com/",
                "service": "accountsettings"
            }
            
            headers = {
                "User-Agent": USER_AGENTS[0],
                "Content-Type": "application/x-www-form-urlencoded"
            }
            
            response = await client.post(url, data=data, headers=headers, follow_redirects=True)
            
            # åˆ†æå“åº”æ¥ç¡®å®šè´¦æˆ·æ˜¯å¦å­˜åœ¨
            account_exists = "identifier" not in response.text.lower() or "doesn't exist" not in response.text.lower()
            
            return {
                "exists": account_exists,
                "email": email,
                "gaia_id": None,  # éœ€è¦è¿›ä¸€æ­¥APIè°ƒç”¨è·å–
                "status": "active" if account_exists else "not_found"
            }
            
    except Exception as e:
        logger.error(f"Google account check error: {str(e)}")
        return {"exists": False, "email": email, "error": str(e)}

async def get_google_profile_info(email: str) -> Dict[str, Any]:
    """
    è·å–Googleå…¬å¼€èµ„æ–™ä¿¡æ¯
    """
    try:
        # æœç´¢Google+èµ„æ–™ä¿¡æ¯ï¼ˆè™½ç„¶Google+å·²å…³é—­ï¼Œä½†æŸäº›æ•°æ®å¯èƒ½ä»ç„¶å¯è®¿é—®ï¼‰
        profile_search = await perform_duckduckgo_search(f'"{email}" site:plus.google.com OR site:profiles.google.com')
        
        profile_info = {
            "name": None,
            "avatar_url": None,
            "last_seen": None,
            "public_info": profile_search
        }
        
        # å°è¯•ä»æœç´¢ç»“æœä¸­æå–ä¿¡æ¯
        for result in profile_search:
            if "google" in result.get("url", "").lower():
                # æå–å¯èƒ½çš„å§“åä¿¡æ¯
                text = result.get("description", "")
                name_match = re.search(r'([A-Z][a-z]+ [A-Z][a-z]+)', text)
                if name_match and not profile_info["name"]:
                    profile_info["name"] = name_match.group(1)
        
        return profile_info
        
    except Exception as e:
        logger.error(f"Google profile info error: {str(e)}")
        return {"error": str(e)}

async def get_google_maps_data(gaia_id: Optional[str]) -> Dict[str, Any]:
    """
    è·å–Google Mapsç›¸å…³æ•°æ®
    """
    if not gaia_id:
        return {"error": "No Gaia ID provided"}
    
    try:
        maps_data = {
            "profile_url": f"https://www.google.com/maps/contrib/{gaia_id}/reviews",
            "reviews_count": 0,
            "photos_count": 0,
            "places_visited": [],
            "reviews": []
        }
        
        # è¿™é‡Œéœ€è¦å®é™…çš„Google Maps APIè°ƒç”¨
        # ç›®å‰è¿”å›æ¨¡æ‹Ÿæ•°æ®ç»“æ„
        
        return maps_data
        
    except Exception as e:
        logger.error(f"Google Maps data error: {str(e)}")
        return {"error": str(e)}

async def search_social_profiles(email: str) -> List[Dict[str, Any]]:
    """
    æœç´¢ç¤¾äº¤åª’ä½“èµ„æ–™
    """
    try:
        social_platforms = [
            "linkedin.com", "facebook.com", "twitter.com", "instagram.com", 
            "github.com", "youtube.com", "pinterest.com"
        ]
        
        social_profiles = []
        
        for platform in social_platforms:
            # æœç´¢ç‰¹å®šå¹³å°ä¸Šçš„èµ„æ–™
            search_query = f'"{email}" site:{platform}'
            platform_results = await perform_duckduckgo_search(search_query, num_results=3)
            
            for result in platform_results:
                if platform in result.get("url", ""):
                    social_profiles.append({
                        "platform": platform.replace(".com", "").title(),
                        "url": result["url"],
                        "title": result["title"],
                        "description": result["description"]
                    })
        
        return social_profiles
        
    except Exception as e:
        logger.error(f"Social profiles search error: {str(e)}")
        return []

async def fetch_google_maps_reviews(gaia_id: str, max_reviews: int) -> List[Dict[str, Any]]:
    """
    è·å–Google Mapsè¯„è®º
    """
    try:
        # è¿™é‡Œéœ€è¦å®é™…çš„Google Maps APIå®ç°
        # ç›®å‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
        reviews = [
            {
                "id": f"review_{i}",
                "rating": 4,
                "text": f"Sample review {i}",
                "date": "2024-01-01",
                "place_name": f"Location {i}",
                "place_id": f"place_{i}"
            }
            for i in range(min(max_reviews, 5))
        ]
        
        return reviews
        
    except Exception as e:
        logger.error(f"Google Maps reviews fetch error: {str(e)}")
        return []

async def fetch_google_avatar(email: str, size: int) -> Dict[str, Any]:
    """
    è·å–Googleè´¦æˆ·å¤´åƒ
    """
    try:
        # å°è¯•æ„å»ºGoogleå¤´åƒURL
        # æ³¨æ„ï¼šè¿™å¯èƒ½ä¸æ€»æ˜¯æœ‰æ•ˆï¼Œå–å†³äºè´¦æˆ·éšç§è®¾ç½®
        
        # æ–¹æ³•1: å°è¯•Gravatarï¼ˆè®¸å¤šGoogleè´¦æˆ·ä¹Ÿä½¿ç”¨Gravatarï¼‰
        import hashlib
        email_hash = hashlib.md5(email.lower().encode()).hexdigest()
        gravatar_url = f"https://www.gravatar.com/avatar/{email_hash}?s={size}&d=404"
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.head(gravatar_url)
            if response.status_code == 200:
                return {
                    "url": gravatar_url,
                    "is_default": False,
                    "source": "gravatar"
                }
        
        # æ–¹æ³•2: ç”Ÿæˆé»˜è®¤å¤´åƒ
        initials = email.split("@")[0][:2].upper()
        default_avatar = f"https://ui-avatars.com/api/?name={initials}&background=4285f4&color=ffffff&size={size}"
        
        return {
            "url": default_avatar,
            "is_default": True,
            "source": "generated"
        }
        
    except Exception as e:
        logger.error(f"Google avatar fetch error: {str(e)}")
        return {
            "url": None,
            "is_default": True,
            "error": str(e)
        }

async def fallback_google_analysis(email: str) -> Dict[str, Any]:
    """
    å¤‡ç”¨Googleåˆ†ææ–¹æ¡ˆ - å½“å¤–éƒ¨APIä¸å¯ç”¨æ—¶ä½¿ç”¨
    """
    try:
        logger.info(f"ğŸ”„ [Google API] Using fallback analysis for: {email}")
        
        # æ‰§è¡ŒåŸºæœ¬çš„Googleè´¦æˆ·æ£€æŸ¥
        account_info = await check_google_account_existence(email)
        profile_info = await get_google_profile_info(email)
        social_profiles = await search_social_profiles(email)
        
        # ç”Ÿæˆå¤´åƒURL
        avatar_info = await fetch_google_avatar(email, 200)
        
        # è¯„ä¼°éšç§é£é™©
        risk_assessment = assess_privacy_risk(
            account_info, profile_info, {}, social_profiles
        )
        
        # æ„å»ºå¤‡ç”¨å“åº”
        result = {
            "email": email,
            "step1_registration": {
                "account_exists": account_info.get("exists", False),
                "gaia_id": account_info.get("gaia_id"),
                "status": account_info.get("status", "unknown")
            },
            "step2_people_info": {
                "name": profile_info.get("name"),
                "public_profiles": profile_info.get("public_info", [])
            },
            "step3_additional_data": {
                "social_profiles": social_profiles,
                "profile_count": len(social_profiles)
            },
            "step4_summary": {
                "total_data_sources": len(social_profiles) + (1 if account_info.get("exists") else 0),
                "privacy_exposure": "Basic analysis based on available data"
            },
            "step5_location_analysis": {
                "location_data": "External API unavailable, location data not accessible",
                "maps_data": {}
            },
            "step6_reverse_image": {
                "avatar_analysis": "Basic avatar generation",
                "reverse_search_results": []
            },
            "avatar_url": avatar_info.get("url"),
            "analysis_timestamp": datetime.utcnow().isoformat(),
            "privacy_score": risk_assessment.get("score", "UNKNOWN"),
            "total_data_points": risk_assessment.get("risk_factors", 0),
            "overall_risk_level": risk_assessment.get("risk_level", "UNKNOWN"),
            "fallback_mode": True,
            "external_api_status": "unavailable - using local analysis"
        }
        
        logger.info(f"âœ… [Google API] Fallback analysis completed for {email}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ [Google API] Fallback analysis failed: {str(e)}")
        # è¿”å›æœ€å°åŒ–çš„é”™è¯¯å“åº”
        return {
            "email": email,
            "error": "Analysis failed",
            "error_detail": str(e),
            "fallback_mode": True,
            "analysis_timestamp": datetime.utcnow().isoformat(),
            "external_api_status": "unavailable"
        }

def assess_privacy_risk(account_info: Dict, profile_info: Dict, maps_data: Dict, social_profiles: List) -> Dict[str, str]:
    """
    è¯„ä¼°éšç§é£é™©
    """
    risk_factors = 0
    
    # è´¦æˆ·å­˜åœ¨æ€§
    if account_info.get("exists"):
        risk_factors += 1
    
    # å…¬å¼€èµ„æ–™ä¿¡æ¯
    if profile_info.get("name"):
        risk_factors += 1
    
    if profile_info.get("avatar_url"):
        risk_factors += 1
    
    # Mapsæ•°æ®
    if maps_data.get("reviews_count", 0) > 0:
        risk_factors += 2
    
    # ç¤¾äº¤åª’ä½“èµ„æ–™
    risk_factors += len(social_profiles)
    
    # è¯„ä¼°é£é™©ç­‰çº§
    if risk_factors == 0:
        score = "VERY_LOW"
        risk_level = "MINIMAL"
    elif risk_factors <= 2:
        score = "LOW" 
        risk_level = "LOW"
    elif risk_factors <= 5:
        score = "MEDIUM"
        risk_level = "MODERATE"
    elif risk_factors <= 8:
        score = "HIGH"
        risk_level = "HIGH"
    else:
        score = "VERY_HIGH"
        risk_level = "CRITICAL"
    
    return {
        "score": score,
        "risk_level": risk_level,
        "risk_factors": risk_factors
    }

async def fallback_email_investigation(email: str) -> Dict[str, Any]:
    """
    å¤‡ç”¨é‚®ç®±è°ƒæŸ¥æ–¹æ¡ˆ - å½“å¤–éƒ¨APIä¸å¯ç”¨æ—¶ä½¿ç”¨
    """
    try:
        logger.info(f"ğŸ”„ [Google Investigate] Using fallback investigation for: {email}")
        
        # æ‰§è¡ŒåŸºæœ¬çš„Googleè´¦æˆ·æ£€æŸ¥
        account_info = await check_google_account_existence(email)
        profile_info = await get_google_profile_info(email)
        social_profiles = await search_social_profiles(email)
        
        # ç”Ÿæˆå¤´åƒURL
        avatar_info = await fetch_google_avatar(email, 200)
        
        # æ„å»ºå¤‡ç”¨å“åº”
        result = {
            "email": email,
            "investigation_status": "fallback_completed",
            "data_sources": ["basic_google_check", "profile_search", "social_search"],
            "profile_data": {
                "account_exists": account_info.get("exists", False),
                "name": profile_info.get("name"),
                "avatar_url": avatar_info.get("url"),
                "public_profiles": profile_info.get("public_info", [])
            },
            "social_accounts": social_profiles,
            "digital_footprint": {
                "search_results_count": len(profile_info.get("public_info", [])),
                "social_presence_count": len(social_profiles)
            },
            "privacy_analysis": {
                "exposure_level": "basic_analysis",
                "risk_score": len(social_profiles) * 10,  # ç®€å•è¯„åˆ†
                "recommendations": [
                    "å®¡æŸ¥ç¤¾äº¤åª’ä½“éšç§è®¾ç½®",
                    "å®šæœŸæ£€æŸ¥å…¬å¼€å¯è§çš„ä¸ªäººä¿¡æ¯",
                    "è€ƒè™‘ä½¿ç”¨éšç§ä¿æŠ¤å·¥å…·"
                ]
            },
            "metadata": {
                "investigation_timestamp": datetime.utcnow().isoformat(),
                "api_version": "fallback_v1",
                "processing_time": "local_processing",
                "data_quality": "basic"
            },
            "external_api_status": "unavailable - using local analysis"
        }
        
        logger.info(f"âœ… [Google Investigate] Fallback investigation completed for {email}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ [Google Investigate] Fallback investigation failed: {str(e)}")
        # è¿”å›æœ€å°åŒ–çš„é”™è¯¯å“åº”
        return {
            "email": email,
            "investigation_status": "failed",
            "error": "Investigation failed",
            "error_detail": str(e),
            "fallback_mode": True,
            "investigation_timestamp": datetime.utcnow().isoformat(),
            "external_api_status": "unavailable"
        }